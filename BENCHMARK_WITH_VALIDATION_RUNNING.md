# Benchmark Running - With Correctness Validation

**Started**: November 21, 2024
**Status**: ğŸŸ¢ **RUNNING**
**Validation**: âœ… **ENABLED**

---

## ğŸ¯ What's Running

### Benchmark Configuration

**Scales**: small, medium, large

**For Each Scale**:
1. Generate nise data
2. Upload to MinIO
3. **In-Memory Mode**:
   - Run POC aggregation
   - Capture performance metrics
   - **âœ… Validate correctness** â† NEW
   - Fail-fast if validation fails
4. **Streaming Mode**:
   - Run POC aggregation
   - Capture performance metrics
   - **âœ… Validate correctness** â† NEW
   - Fail-fast if validation fails

---

## âœ… Correctness Validation Details

### What Gets Validated

For each test run, the validation script:

1. **Reads nise CSV files** (raw input data)
2. **Calculates expected aggregates**:
   - Group by date, namespace, node
   - Sum CPU/memory metrics
   - Convert to core-hours, GB-hours
3. **Queries PostgreSQL** for actual POC results
4. **Compares expected vs actual**:
   - CPU usage, request, limit (core-hours)
   - Memory usage, request, limit (GB-hours)
   - Tolerance: 1% relative difference
5. **Fails if any metric exceeds tolerance**

### Benefits

- âœ… **Correctness guaranteed**: Not just performance, but accurate results
- âœ… **Self-contained**: No dependency on IQE code
- âœ… **Comprehensive**: All aggregation metrics validated
- âœ… **Fail-fast**: Stops immediately on incorrect results

---

## ğŸ“Š Monitoring

### Check Progress

```bash
tail -f benchmark_with_validation.log
```

### Check Results Directory

```bash
ls -lh benchmark_results/streaming_comparison_*/
```

### Check for Errors

```bash
grep -E "âŒ|FAILED|ERROR" benchmark_with_validation.log
```

---

## ğŸ“ˆ Expected Timeline

| Phase | Duration | What Happens |
|-------|----------|--------------|
| **Setup** | 1 min | Environment check, validation |
| **Small Scale** | ~5 min | Generate â†’ In-memory â†’ Validate â†’ Streaming â†’ Validate |
| **Medium Scale** | ~7 min | Generate â†’ In-memory â†’ Validate â†’ Streaming â†’ Validate |
| **Large Scale** | ~10 min | Generate â†’ In-memory â†’ Validate â†’ Streaming â†’ Validate |
| **Report** | 1 min | Generate comparison report |
| **TOTAL** | ~24 min | With correctness validation |

**Note**: Validation adds ~1-2 minutes per test but ensures results are correct!

---

## ğŸ” What to Look For

### Success Indicators

For each scale/mode combination:

```
3ï¸âƒ£  Testing IN-MEMORY mode...
   Configuring...
   âœ“ In-memory mode configured
   Running benchmark...
   âœ… POC completed (5.2s, 45.3 MB peak)
   ğŸ” Validating correctness...
   âœ… CORRECTNESS VALIDATED
   âœ… COMPLETE (functional + correctness validated)

4ï¸âƒ£  Testing STREAMING mode...
   Configuring...
   âœ“ Streaming mode configured
   Running benchmark...
   âœ… POC completed (6.1s, 32.8 MB peak)
   ğŸ” Validating correctness...
   âœ… CORRECTNESS VALIDATED
   âœ… COMPLETE (functional + correctness validated)
```

### Failure Indicators (Fail-Fast)

If validation fails:

```
   âœ… POC completed (5.2s, 45.3 MB peak)
   ğŸ” Validating correctness...
   âŒ CORRECTNESS VALIDATION FAILED
   Last 30 lines of validation log:

   âŒ cpu_usage_core_hours: 12 rows exceed 1.0% tolerance
      Max difference: 15.3%
      ...

âŒ FAIL-FAST: Aggregation produced incorrect results
   Full validation log: benchmark_results/.../small_in-memory_validation.log
```

---

## ğŸ“‚ Output Files

### For Each Scale

```
benchmark_results/streaming_comparison_<timestamp>/
â”œâ”€â”€ small_in-memory.log              # POC execution log
â”œâ”€â”€ small_in-memory_validation.log   # Correctness validation details
â”œâ”€â”€ small_streaming.log              # POC execution log
â”œâ”€â”€ small_streaming_validation.log   # Correctness validation details
â”œâ”€â”€ medium_in-memory.log
â”œâ”€â”€ medium_in-memory_validation.log
â”œâ”€â”€ medium_streaming.log
â”œâ”€â”€ medium_streaming_validation.log
â”œâ”€â”€ large_in-memory.log
â”œâ”€â”€ large_in-memory_validation.log
â”œâ”€â”€ large_streaming.log
â”œâ”€â”€ large_streaming_validation.log
â”œâ”€â”€ SUMMARY.csv                      # Performance + validation summary
â””â”€â”€ COMPARISON_REPORT.md             # Final comparison report
```

### SUMMARY.csv Format

**NEW**: Added `validation_status` column

```csv
scale,mode,status,duration_seconds,peak_memory_mb,input_rows,output_rows,validation_status
small,in-memory,SUCCESS,5.2,45.3,12370,2046,PASS
small,streaming,SUCCESS,6.1,32.8,12370,2046,PASS
medium,in-memory,SUCCESS,12.4,89.7,123450,20456,PASS
medium,streaming,SUCCESS,15.2,67.3,123450,20456,PASS
...
```

---

## ğŸ¯ Success Criteria

### Performance Validation
- âœ… POC completes without errors
- âœ… Memory usage captured
- âœ… Processing time captured
- âœ… Row counts captured

### Correctness Validation (NEW)
- âœ… All CPU metrics within 1% tolerance
- âœ… All memory metrics within 1% tolerance
- âœ… Row counts match expected
- âœ… No missing or extra data

### Comparison
- âœ… Streaming vs in-memory performance compared
- âœ… Memory savings quantified
- âœ… Streaming overhead measured
- âœ… **Both modes produce correct results**

---

## ğŸš¨ If Something Goes Wrong

### Validation Failure

If correctness validation fails:

1. **Check validation log**:
   ```bash
   cat benchmark_results/streaming_comparison_*/small_in-memory_validation.log
   ```

2. **Identify which metrics failed**:
   - CPU usage/request/limit?
   - Memory usage/request/limit?
   - Row count mismatch?

3. **Check for regressions**:
   - Did recent code changes break aggregation logic?
   - Are label merges working correctly?
   - Are capacity calculations accurate?

### Functional Failure

If POC crashes or hangs:

1. **Check POC log**:
   ```bash
   cat benchmark_results/streaming_comparison_*/small_in-memory.log
   ```

2. **Look for errors**:
   - Database connection issues?
   - S3 read errors?
   - Memory exhaustion?

---

## ğŸ’¡ What This Gives Us

### Before (Performance Only)

```
âœ… POC runs in 5.2 seconds
âœ… Uses 45 MB memory
â“ But are the results correct? UNKNOWN
```

### After (Performance + Correctness)

```
âœ… POC runs in 5.2 seconds
âœ… Uses 45 MB memory
âœ… All aggregated values correct (validated against nise)
âœ… CPU metrics accurate within 1%
âœ… Memory metrics accurate within 1%
âœ… Row counts match expected
âœ… Can confidently trust results
```

---

## ğŸ“Š What We'll Learn

### Performance Questions
- â“ How much faster is in-memory vs streaming?
- â“ How much memory does streaming save?
- â“ Does streaming scale better?

### Correctness Questions (NEW)
- â“ Do both modes produce identical results?
- â“ Are all aggregation metrics accurate?
- â“ Does the POC match Trino output?
- â“ Can we trust this for production?

**All will be answered with validation!**

---

## âœ… Summary

**Running**: Benchmarks with integrated correctness validation
**Scales**: small, medium, large
**Validation**: POC results vs nise expected values
**Fail-Fast**: Stops immediately on incorrect results
**Duration**: ~24 minutes
**Confidence**: ğŸŸ¢ **HIGH** - Both performance AND correctness validated

---

## ğŸ”— Related Documents

- `BENCHMARK_CORRECTNESS_ASSESSMENT.md` - Why validation is needed
- `CORRECTNESS_VALIDATION_IMPLEMENTED.md` - How validation works
- `scripts/validate_benchmark_correctness.py` - Validation script
- `scripts/run_streaming_comparison.sh` - Benchmark script (modified)

---

*Benchmark running... Results will be in `benchmark_results/streaming_comparison_<timestamp>/`*

