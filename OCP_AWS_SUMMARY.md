# OCP on AWS Provider - Executive Summary

## Quick Overview

**Goal**: Implement custom Parquet aggregator for OCP on AWS provider to replace Trino + Hive

**Status**: Triage Complete âœ…

**Complexity**: **HIGH** (3.3x more complex than standalone OCP)

**Estimated Effort**: **3-4 weeks**

**Feasibility**: âœ… **FEASIBLE**

---

## What is OCP on AWS?

OCP on AWS combines two data sources:

1. **OpenShift (OCP)** - Container/pod resource usage
2. **AWS** - Cloud infrastructure costs

The system **matches** OCP workloads to AWS resources (EC2, EBS, RDS, etc.) to show:
- Which AWS resources are used by which OpenShift namespaces/projects
- How much each namespace costs in AWS infrastructure

---

## Key Differences from Standalone OCP

| Aspect | OCP POC | OCP+AWS POC |
|--------|---------|-------------|
| **Data Sources** | 1 (OCP only) | 2 (OCP + AWS) |
| **Matching Logic** | None | Resource ID + Tag matching |
| **Cost Types** | Simple | 4 types (unblended, blended, savings plan, amortized) |
| **Aggregation Tables** | 1 | 9 |
| **SQL Files** | 4 | 13 |
| **Complexity** | LOW | HIGH |
| **Estimated Effort** | 2 weeks | 3-4 weeks |

---

## Core Algorithms

### 1. Resource ID Matching â­

**What**: Match AWS resources to OCP by instance/volume IDs

**Example**:
```
AWS EC2 Instance: i-0123456789abcdef0
OCP Node: ip-10-0-1-100.ec2.internal (resource_id: i-0123456789abcdef0)
â†’ MATCH âœ…
```

**Complexity**: HIGH (8/10)

### 2. Tag Matching â­

**What**: Match AWS resources to OCP by special tags

**Tags**:
- `openshift_cluster: my-cluster`
- `openshift_node: worker-1`
- `openshift_project: my-namespace`

**Complexity**: MEDIUM-HIGH (7/10)

### 3. Disk Capacity Calculation â­

**What**: Calculate EBS volume capacity for storage cost attribution

**Formula**:
```
Capacity (GB) = Total Cost / (Hourly Rate / Hours in Month)
```

**Complexity**: VERY HIGH (9/10) - Most complex algorithm

### 4. Network Cost Attribution

**What**: Handle node-level network costs that can't be attributed to specific pods

**Solution**: Assign to special namespace "Network unattributed"

**Complexity**: MEDIUM-HIGH (7/10)

### 5. Label Precedence

**What**: Apply Pod > Namespace > Node label precedence (same as OCP POC)

**Status**: âœ… Already implemented in OCP POC

**Complexity**: LOW (reuse existing)

---

## Trino SQL Files to Replace

### Phase 1: Daily Summary Population (4 files)

1. **`0_prepare_daily_summary_tables.sql`**
   - Creates Hive temp tables
   - **Complexity**: LOW

2. **`1_resource_matching_by_cluster.sql`** â­ **CRITICAL**
   - Matches AWS resources to OCP clusters
   - Resource ID matching + Tag matching
   - **Complexity**: HIGH
   - **Lines**: ~173

3. **`2_summarize_data_by_cluster.sql`** â­ **CRITICAL**
   - Joins OCP usage with AWS costs
   - Disk capacity calculation
   - Network cost handling
   - **Complexity**: VERY HIGH
   - **Lines**: ~900

4. **`3_reporting_ocpawscostlineitem_project_daily_summary_p.sql`**
   - Merges temp data into final table
   - **Complexity**: LOW

### Phase 2: PostgreSQL Aggregations (9 files)

All follow similar pattern but with different dimensions:

1. **Detailed line items** - All dimensions
2. **Cluster totals** - Cluster only
3. **By account** - AWS account
4. **By service** - AWS service/product
5. **By region** - AWS region/AZ
6. **Compute** - EC2 instances
7. **Storage** - Storage services
8. **Database** - RDS, DynamoDB, etc.
9. **Network** - VPC, CloudFront, etc.

**Complexity**: LOW-MEDIUM (similar patterns)

---

## Implementation Phases

### Week 1: Core Infrastructure + Tag Matching

**Tasks**:
- âœ… AWS data loading from Parquet
- âœ… Resource ID matching algorithm
- âœ… Tag filtering and matching
- âœ… Combine matching results

**Deliverables**:
- `src/aws_data_loader.py`
- `src/resource_matcher.py`
- `src/tag_matcher.py`

### Week 2: Cost Attribution + Special Cases

**Tasks**:
- âœ… Join OCP usage with AWS costs
- âœ… Disk capacity calculation
- âœ… Storage cost attribution
- âœ… Network cost handling
- âœ… Markup application

**Deliverables**:
- `src/cost_attributor.py`
- `src/disk_capacity_calculator.py`
- `src/network_cost_handler.py`

### Week 3: Aggregation + Integration

**Tasks**:
- âœ… Implement 9 aggregation tables
- âœ… Main pipeline orchestration
- âœ… IQE test suite adaptation
- âœ… Performance optimization

**Deliverables**:
- `src/aggregator_ocpaws.py`
- `src/main_ocpaws.py`
- `src/iqe_validator_ocpaws.py`

### Week 4: Testing + Documentation

**Tasks**:
- âœ… Comprehensive testing
- âœ… Validation against Trino
- âœ… Performance benchmarks
- âœ… Technical documentation

**Deliverables**:
- Test results
- Performance analysis
- Technical architecture document

---

## Risk Assessment

### ðŸ”´ High Risks

1. **Disk Capacity Calculation Accuracy**
   - Complex formula, must match Trino exactly
   - **Mitigation**: Extensive validation with real data

2. **Resource ID Matching Edge Cases**
   - Some AWS resources may not match
   - **Mitigation**: Comprehensive test coverage, fallback to tag matching

3. **Performance with Large Datasets**
   - AWS CUR data can be millions of rows
   - **Mitigation**: Streaming mode, parallel processing

### ðŸŸ¡ Medium Risks

4. **Tag Matching Complexity**
   - JSON parsing, dynamic enabled keys
   - **Mitigation**: Unit tests, validation

5. **Multiple Cost Types**
   - 4 different cost types (unblended, blended, SP, amortized)
   - **Mitigation**: Clear documentation, validation for each

### ðŸŸ¢ Low Risks

6. **Aggregation Logic**
   - Standard SQL patterns
   - **Mitigation**: Reuse OCP POC patterns

---

## Success Criteria

### Functional Correctness (Must Have)

âœ… Cluster-level totals match Trino (within 0.01%)
âœ… Namespace-level totals match Trino (within 0.1%)
âœ… All 9 aggregation tables match Trino
âœ… Resource ID matching accuracy > 99%
âœ… Tag matching accuracy > 99%
âœ… All IQE test scenarios pass

### Performance (Should Have)

âœ… Memory usage < 2x Trino
âœ… Processing time < 3x Trino
âœ… Handles 100K AWS line items without OOM
âœ… Streaming mode works for large datasets

---

## Effort Breakdown

| Phase | Days | % of Total |
|-------|------|------------|
| Core Infrastructure | 7 | 21% |
| Tag Matching | 6 | 18% |
| Cost Attribution | 8 | 24% |
| Network & Special Cases | 6 | 18% |
| Aggregation | 7 | 21% |
| Integration & Testing | 10 | 30% |
| Documentation | 4 | 12% |
| **TOTAL** | **48** | **100%** |

**Note**: Includes risk buffer. Actual development may be faster with reuse from OCP POC.

---

## Comparison: OCP vs. OCP+AWS

| Metric | OCP POC | OCP+AWS POC | Multiplier |
|--------|---------|-------------|------------|
| **Lines of Code** | ~2,000 | ~4,000 (est.) | 2x |
| **SQL Files** | 4 | 13 | 3.25x |
| **Aggregation Tables** | 1 | 9 | 9x |
| **Development Days** | 10 | 33 | 3.3x |
| **Algorithms** | 2 (label precedence, capacity) | 6 (matching, capacity, network, etc.) | 3x |
| **Data Sources** | 1 | 2 | 2x |
| **Complexity** | LOW | HIGH | - |

---

## Reusable Components from OCP POC

âœ… **Parquet Reader** - Streaming, parallel reading, columnar filtering
âœ… **Label Precedence** - Already implemented in OCP aggregation
âœ… **Memory Optimization** - Categorical types, chunking, GC
âœ… **PostgreSQL Writer** - Bulk inserts
âœ… **Configuration Management** - YAML config
âœ… **Performance Profiling** - Benchmark scripts
âœ… **Logging and Error Handling** - Utils

**Estimated Reuse**: ~30-40% of OCP POC code

---

## New Components to Build

âŒ **AWS Data Loader** - Read AWS CUR Parquet files
âŒ **Resource Matcher** - Match AWS to OCP by resource ID
âŒ **Tag Matcher** - Match AWS to OCP by tags
âŒ **Cost Attributor** - Join OCP usage with AWS costs
âŒ **Disk Capacity Calculator** - Calculate EBS volume capacities
âŒ **Network Cost Handler** - Handle node-level network costs
âŒ **OCP+AWS Aggregator** - 9 aggregation tables
âŒ **Main Pipeline** - Orchestrate all phases
âŒ **IQE Validator** - Validate OCP+AWS results

**Estimated New Code**: ~60-70% of total

---

## Data Schema Highlights

### Source: AWS Line Items (Parquet)

**Key Columns**:
- `lineitem_resourceid` - AWS resource ID (e.g., i-0123..., vol-0123...)
- `lineitem_usageaccountid` - AWS account
- `lineitem_productcode` - AWS service (EC2, RDS, etc.)
- `product_instancetype` - EC2 instance type
- `lineitem_unblendedcost` - Standard cost
- `lineitem_blendedcost` - Blended cost
- `savingsplan_savingsplaneffectivecost` - Savings Plan cost
- `resourcetags` - JSON tags
- Partitions: `source`, `year`, `month`

### Intermediate: OCP+AWS Combined (Parquet)

**Key Columns**:
- **OCP**: cluster, namespace, node, pod, PVC, PV
- **AWS**: resource_id, account, region, product, instance_type
- **Costs**: unblended, blended, savings plan, amortized (+ markup for each)
- **Matching**: resource_id_matched, matched_tag
- **Labels**: pod_labels (with precedence), tags (AWS)
- Partitions: `source`, `ocp_source`, `year`, `month`, `day`

### Target: 9 PostgreSQL Tables

All include:
- Dimensions (varies by table)
- 4 cost types Ã— 2 (cost + markup) = 8 cost columns
- Metadata (cluster, source_uuid, etc.)

---

## Testing Strategy

### Unit Tests

**Coverage**: All algorithms

**Key Tests**:
- Resource ID matching (nodes, volumes, edge cases)
- Tag matching (enabled keys, precedence)
- Disk capacity calculation (various rates)
- Network direction calculation
- Cost attribution (capacity ratios)

### Integration Tests

**Coverage**: End-to-end pipeline

**Scenarios**:
- Simple: 1 cluster, 1 account, resource ID only
- Complex: Multiple clusters, mixed matching
- Storage: PVCs with EBS volumes
- Network: Node-level costs
- Database: RDS with tag matching

### IQE Validation

**Coverage**: Real-world scenarios

**Approach**:
1. Generate test data (nise or synthetic)
2. Run Trino (baseline)
3. Run POC
4. Compare results

**Expected**: 100% match on cluster totals, 99%+ on namespace totals

---

## Performance Expectations (With Optimizations)

### Memory Usage

| Scale | AWS Rows | OCP Rows | Memory (Standard) | Memory (Streaming) | Container |
|-------|----------|----------|-------------------|-------------------|-----------|
| **Small** | 1K | 1K | 100-200 MB | N/A | 1 GB |
| **Medium** | 10K | 10K | 500-800 MB | N/A | 2 GB |
| **Large** | 100K | 100K | 2-3 GB | N/A | 4 GB |
| **XL** | 1M | 1M | 10-15 GB | 3-4 GB | 8 GB |
| **XXL** | 10M | 10M | âŒ Not feasible | 3-4 GB | 8 GB |

### Processing Time

**Expected**: **2-3x FASTER** than Trino (with parallel reading)

**With Streaming**: 10-20% slower than standard mode, but enables unlimited scale

### Performance Optimizations (From OCP POC)

Based on successful OCP POC implementation:

#### 1. âœ… Streaming Mode (80-90% Memory Savings)
- Process in 50K row chunks
- **Impact**: 10M rows = 3 GB (vs. 100 GB)

#### 2. âœ… Parallel File Reading (2-4x Speedup)
- 4 concurrent readers
- **Impact**: 3.3x faster file reading

#### 3. âœ… Columnar Filtering (30-40% Memory Savings)
- Read only essential columns
- **Impact**: 40% less memory

#### 4. âœ… Categorical Types (50-70% String Memory Savings)
- Convert strings to categories
- **Impact**: 71% less memory for string columns

#### 5. âœ… Memory Cleanup (10-20% Peak Reduction)
- Explicit garbage collection
- **Impact**: Immediate memory release

#### 6. âœ… Batch Writes (10-50x Speedup)
- Batch PostgreSQL inserts
- **Impact**: 50x faster database writes

### Configuration

```yaml
performance:
  parallel_readers: 4
  use_streaming: false  # Enable for > 1M rows
  use_categorical: true
  column_filtering: true
  chunk_size: 50000
  gc_after_aggregation: true
  delete_intermediate_dfs: true
  db_batch_size: 1000
```

### Scalability Verdict

âœ… **POC scales better than Trino for 90% of deployments**
- 10-1000x cheaper
- 2-3x faster (with optimizations)
- Constant memory (streaming mode)
- Simpler operations

---

## Recommendations

### âœ… Proceed with Implementation

**Rationale**:
1. Complexity is manageable with phased approach
2. Can reuse 30-40% from OCP POC
3. Core algorithms are well-defined
4. Risk mitigation strategies in place

### ðŸŽ¯ Start with Phase 1

**Immediate Next Steps**:
1. Implement AWS data loader
2. Build resource ID matching
3. Validate matching logic against Trino
4. Create simple test scenario

### ðŸ“Š Validate Early and Often

**Strategy**:
1. Validate matching counts after Phase 1
2. Validate cost attribution after Phase 3
3. Validate each aggregation table after Phase 5
4. Full IQE validation after Phase 6

### ðŸš€ Performance from Day 1

**Approach**:
1. Use streaming mode from start
2. Implement chunking for AWS data
3. Monitor memory continuously
4. Benchmark at each phase

---

## Questions to Resolve

### 1. Data Generation

**Question**: Can nise generate OCP+AWS combined scenarios?

**Options**:
- A) Use nise if supported
- B) Build synthetic generator
- C) Use production data sample

**Recommendation**: Investigate nise first, fallback to synthetic

### 2. Disk Capacity Data Source

**Question**: Do we need hourly AWS data or can we use daily?

**Impact**: Capacity calculation requires hourly rates

**Recommendation**: Confirm with Trino SQL analysis

### 3. Tag Matching Scope

**Question**: Which tag keys should be enabled by default?

**Impact**: Affects matching coverage

**Recommendation**: Query PostgreSQL `reporting_enabledtagkeys` table

---

## Conclusion

**Status**: âœ… **READY TO PROCEED**

**Confidence Level**: **HIGH** (85%)

**Key Success Factors**:
1. Accurate matching algorithms
2. Correct disk capacity calculation
3. Performance optimization
4. Comprehensive validation

**Biggest Challenges**:
1. Disk capacity calculation (complexity 9/10)
2. Resource ID matching edge cases (complexity 8/10)
3. Performance with large datasets (risk: HIGH)

**Expected Outcome**: Fully functional OCP+AWS aggregator that matches Trino results within 0.1% tolerance, with acceptable performance (2-3x Trino).

---

**Next Step**: Review this triage with technical lead, then proceed with Phase 1 implementation.

---

**Document**: OCP_AWS_SUMMARY.md
**Related**: OCP_AWS_TRIAGE.md (detailed triage)
**Date**: November 21, 2025
**Status**: Ready for Review

