#!/usr/bin/env python3
"""Convert nise CSV files to Parquet and upload to MinIO.

This script reads OCP CSV files generated by nise, converts them to Parquet format,
and uploads them to MinIO in the correct directory structure expected by the POC aggregator.
"""

import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import s3fs
from pathlib import Path
import os
import sys
from datetime import datetime

def main():
    # Configuration from environment
    MINIO_ENDPOINT = os.getenv('S3_ENDPOINT', 'http://localhost:9000')
    MINIO_ACCESS_KEY = os.getenv('S3_ACCESS_KEY', 'minioadmin')
    MINIO_SECRET_KEY = os.getenv('S3_SECRET_KEY', 'minioadmin')
    BUCKET = os.getenv('S3_BUCKET', 'cost-management')
    ORG_ID = '1234567'
    PROVIDER_UUID = os.getenv('OCP_PROVIDER_UUID', '00000000-0000-0000-0000-000000000001')

    # CSV input directory
    csv_dir_arg = sys.argv[1] if len(sys.argv) > 1 else '/tmp/nise-poc-data'
    CSV_DIR = Path(csv_dir_arg)

    if not CSV_DIR.exists():
        print(f"‚ùå Directory not found: {CSV_DIR}")
        sys.exit(1)

    print("=" * 80)
    print("CSV to Parquet Converter for MinIO")
    print("=" * 80)
    print(f"CSV Directory:  {CSV_DIR}")
    print(f"MinIO Endpoint: {MINIO_ENDPOINT}")
    print(f"Bucket:         {BUCKET}")
    print(f"Org ID:         {ORG_ID}")
    print(f"Provider UUID:  {PROVIDER_UUID}")
    print("=" * 80)
    print()

    # S3 filesystem
    try:
        fs = s3fs.S3FileSystem(
            key=MINIO_ACCESS_KEY,
            secret=MINIO_SECRET_KEY,
            client_kwargs={
                'endpoint_url': MINIO_ENDPOINT,
                'region_name': 'us-east-1'
            },
            use_ssl=False
        )
        print("‚úì Connected to MinIO")
    except Exception as e:
        print(f"‚ùå Failed to connect to MinIO: {e}")
        sys.exit(1)

    # Find all OCP CSV files (support both naming patterns)
    file_types = {
        'pod_usage': (['*ocp_pod_usage.csv', '**/*openshift_report.*.csv'], 'openshift_pod_usage_line_items'),
        'node_labels': (['*ocp_node_label.csv', '**/*openshift_node_labels.*.csv'], 'openshift_node_labels_line_items'),
        'namespace_labels': (['*ocp_namespace_label.csv', '**/*openshift_namespace_labels.*.csv'], 'openshift_namespace_labels_line_items'),
    }

    for file_type, (patterns, s3_dir) in file_types.items():
        csv_files = []
        for pattern in patterns:
            csv_files.extend(list(CSV_DIR.glob(pattern)))

        if not csv_files:
            print(f"‚ö†Ô∏è  No {file_type} CSV files found (patterns: {patterns})")
            continue

        print(f"\nüìÑ Processing {file_type}: found {len(csv_files)} file(s)")

        # Combine all CSV files for this type
        all_dfs = []
        for csv_file in csv_files:
            try:
                df_temp = pd.read_csv(csv_file)
                if len(df_temp) > 0:
                    all_dfs.append(df_temp)
                    print(f"   ‚úì {csv_file.name}: {len(df_temp)} rows")
                else:
                    print(f"   ‚ö†Ô∏è  {csv_file.name}: empty, skipping")
            except Exception as e:
                print(f"   ‚ùå {csv_file.name}: failed to read - {e}")

        if not all_dfs:
            print(f"   ‚ö†Ô∏è  No valid data found for {file_type}")
            continue

        df = pd.concat(all_dfs, ignore_index=True)
        print(f"   Total combined rows: {len(df)}")

        # Parse dates and group by day
        # Handle nise date format: "2025-11-01 00:00:00 +0000 UTC"
        # Strip timezone suffix and parse
        df['interval_start_str'] = df['interval_start'].str.replace(r' \+\d{4} UTC$', '', regex=True)
        df['interval_start_parsed'] = pd.to_datetime(df['interval_start_str'])
        df['year_num'] = df['interval_start_parsed'].dt.year
        df['month_num'] = df['interval_start_parsed'].dt.month
        df['day_num'] = df['interval_start_parsed'].dt.day
        df.drop('interval_start_str', axis=1, inplace=True)
        df.drop('interval_start_parsed', axis=1, inplace=True)

        # Group by year, month, day (not just day)
        date_groups = df.groupby(['year_num', 'month_num', 'day_num'])
        if len(date_groups) == 0:
            print(f"   ‚ö†Ô∏è  No valid days found, skipping")
            continue

        print(f"   Date groups found: {len(date_groups)}")

        # Process each date group
        for (year, month, day), group_df in date_groups:
            day_df = group_df.copy()
            day_df.drop(['year_num', 'month_num', 'day_num'], axis=1, inplace=True)

            # S3 path (matching POC aggregator expectations)
            s3_path = f"{BUCKET}/data/{ORG_ID}/OCP/source={PROVIDER_UUID}/year={year}/month={month:02d}/day={day:02d}/{s3_dir}/data.parquet"

            print(f"   {year}-{month:02d}-{day:02d}: {len(day_df)} rows ‚Üí {s3_dir}/")

            try:
                # Convert to Parquet with explicit schema (no dictionary encoding)
                # This prevents type conflicts when reading multiple files
                schema = pa.Schema.from_pandas(day_df)

                # Replace any dictionary-encoded fields with plain string types
                new_fields = []
                for field in schema:
                    if pa.types.is_dictionary(field.type):
                        # Convert dictionary type to plain string
                        new_fields.append(pa.field(field.name, pa.string()))
                    else:
                        new_fields.append(field)

                schema = pa.schema(new_fields)
                table = pa.Table.from_pandas(day_df, schema=schema)

                # Write to MinIO
                with fs.open(s3_path, 'wb') as f:
                    pq.write_table(table, f)

                print(f"      ‚úì Uploaded")
            except Exception as e:
                print(f"      ‚ùå Failed to upload: {e}")

    print()
    print("=" * 80)
    print("‚úì All files converted and uploaded to MinIO")
    print("=" * 80)
    print()
    print("Next steps:")
    print("  1. Verify files in MinIO console: http://localhost:9001")
    print("  2. Run POC aggregator: python3 -m src.main --truncate")
    print()

if __name__ == '__main__':
    main()

